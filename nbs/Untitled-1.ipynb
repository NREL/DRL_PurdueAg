{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\") \n",
    "\n",
    "\n",
    "\n",
    "env.reset() # Instantiate enviroment with default parameters\n",
    "for step in range(300):\n",
    "    env.render() # Show agent actions on screen\n",
    "    env.step(env.action_space.sample()) # Sample random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(300):\n",
    "    env.render() # Show agent actions on screen\n",
    "    env.step(env.action_space.sample()) # Sample random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib import agents\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from controller_env import OptimControllerEnv\n",
    "# import seaborn as sns\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=11513)\u001b[0m 2023-12-12 11:17:57,504\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=11513)\u001b[0m 2023-12-12 11:17:57,504\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=11513)\u001b[0m 2023-12-12 11:17:57,504\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=11513)\u001b[0m 2023-12-12 11:17:57,504\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=11513)\u001b[0m 2023-12-12 11:18:03,300\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=11513)\u001b[0m 2023-12-12 11:18:04,541\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "    'PPO',\n",
    "    stop={\n",
    "        'timesteps_total': 100000\n",
    "    },\n",
    "    config={\n",
    "    \"env\": 'LunarLander-v2',\n",
    "    \"num_workers\": 3,\n",
    "    # \"gamma\" : 0.8,\n",
    "    # \"lr\": 0.001,\n",
    "    },\n",
    "    log_to_file=True,\n",
    "    verbose=0,\n",
    "    local_dir=\"./Lunar_lander/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PurdueAg/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py:281: UserWarning: Dataframes will use '/' instead of '.' to delimit nested result keys in future versions of Ray. For forward compatibility, set the environment variable TUNE_RESULT_DELIM='/'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>agent_timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>...</th>\n",
       "      <th>info.learner.default_policy.learner_stats.total_loss</th>\n",
       "      <th>info.learner.default_policy.learner_stats.policy_loss</th>\n",
       "      <th>info.learner.default_policy.learner_stats.vf_loss</th>\n",
       "      <th>info.learner.default_policy.learner_stats.vf_explained_var</th>\n",
       "      <th>info.learner.default_policy.learner_stats.kl</th>\n",
       "      <th>info.learner.default_policy.learner_stats.entropy</th>\n",
       "      <th>info.learner.default_policy.learner_stats.entropy_coeff</th>\n",
       "      <th>config.evaluation_config.tf_session_args.gpu_options.allow_growth</th>\n",
       "      <th>config.evaluation_config.tf_session_args.device_count.CPU</th>\n",
       "      <th>config.evaluation_config.multiagent.policies.default_policy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c31ea_00000</th>\n",
       "      <td>247.062932</td>\n",
       "      <td>-191.78823</td>\n",
       "      <td>26.256983</td>\n",
       "      <td>407.95</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>103974</td>\n",
       "      <td>7998</td>\n",
       "      <td>103974</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1724.311157</td>\n",
       "      <td>-0.009212</td>\n",
       "      <td>1724.31897</td>\n",
       "      <td>0.303234</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>1.026262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>(&lt;class 'ray.rllib.policy.tf_policy_template.P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "trial_id                                                                   \n",
       "c31ea_00000          247.062932          -191.78823            26.256983   \n",
       "\n",
       "             episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
       "trial_id                                                                 \n",
       "c31ea_00000            407.95                  22                    3   \n",
       "\n",
       "             timesteps_total  timesteps_this_iter  agent_timesteps_total  \\\n",
       "trial_id                                                                   \n",
       "c31ea_00000           103974                 7998                 103974   \n",
       "\n",
       "             done  ...  info.learner.default_policy.learner_stats.total_loss  \\\n",
       "trial_id           ...                                                         \n",
       "c31ea_00000  True  ...                                        1724.311157      \n",
       "\n",
       "             info.learner.default_policy.learner_stats.policy_loss  \\\n",
       "trial_id                                                             \n",
       "c31ea_00000                                          -0.009212       \n",
       "\n",
       "            info.learner.default_policy.learner_stats.vf_loss  \\\n",
       "trial_id                                                        \n",
       "c31ea_00000                                        1724.31897   \n",
       "\n",
       "            info.learner.default_policy.learner_stats.vf_explained_var  \\\n",
       "trial_id                                                                 \n",
       "c31ea_00000                                           0.303234           \n",
       "\n",
       "             info.learner.default_policy.learner_stats.kl  \\\n",
       "trial_id                                                    \n",
       "c31ea_00000                                      0.007208   \n",
       "\n",
       "             info.learner.default_policy.learner_stats.entropy  \\\n",
       "trial_id                                                         \n",
       "c31ea_00000                                           1.026262   \n",
       "\n",
       "             info.learner.default_policy.learner_stats.entropy_coeff  \\\n",
       "trial_id                                                               \n",
       "c31ea_00000                                                0.0         \n",
       "\n",
       "             config.evaluation_config.tf_session_args.gpu_options.allow_growth  \\\n",
       "trial_id                                                                         \n",
       "c31ea_00000                                               True                   \n",
       "\n",
       "            config.evaluation_config.tf_session_args.device_count.CPU  \\\n",
       "trial_id                                                                \n",
       "c31ea_00000                                                  1          \n",
       "\n",
       "            config.evaluation_config.multiagent.policies.default_policy  \n",
       "trial_id                                                                 \n",
       "c31ea_00000  (<class 'ray.rllib.policy.tf_policy_template.P...           \n",
       "\n",
       "[1 rows x 348 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PurdueAg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
